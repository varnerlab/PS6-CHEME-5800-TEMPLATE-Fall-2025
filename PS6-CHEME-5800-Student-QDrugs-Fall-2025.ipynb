{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9215e205",
   "metadata": {},
   "source": [
    "# L13b: Drug Combination Design using Reinforcement Learning\n",
    "In this lab, we apply Q-learning to design drug combinations that maximize therapeutic effectiveness while satisfying budget constraints. The goal is to learn an optimal policy that selects drug types and dosage levels through repeated interaction with an environment.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> * **Implement Q-learning for constrained optimization**: Build a Q-learning agent that learns state-action value functions through temporal difference updates, demonstrating how reinforcement learning discovers optimal policies without explicit knowledge of environment dynamics.\n",
    "> * **Model combinatorial drug design problems**: Encode drug cocktail design as a Markov decision process with discrete state spaces (dosage levels) and action spaces (drug selection), and apply Cobb-Douglas utility functions to evaluate therapeutic effectiveness under budget constraints.\n",
    "> * **Extract policies from learned Q-values**: Derive optimal drug selection and dosage policies by analyzing converged Q-value tables, mapping each state (current dosage configuration) to the action (drug combination) that maximizes expected cumulative reward.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f2c7e",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The Drug Cocktail Design Problem involves selecting a combination of drugs to create a cocktail that maximizes therapeutic effectiveness while staying within budget and safety constraints. Each drug has attributes such as dosage, mechanism of action, and side effects, which contribute to the overall effectiveness of the cocktail. The drug cocktail design problem can be formulated as the following optimization problem:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{maximize} \\quad & U(n_1, \\dots, n_K) = \\kappa(\\gamma) \\prod_{i=1}^{K} n_i^{\\gamma_i} \\\\\n",
    "\\text{subject to} \\quad & \\sum_{i=1}^{K} c_i (n_i\\;W) \\leq B\\quad\\text{(budget constraint)}\\\\\n",
    "& \\sum_{i=1}^{K} n_i \\leq S\\quad\\text{(safety constraint)}\\\\\n",
    "& n_i^{\\text{min}} \\leq n_i \\leq n_i^{\\text{max}} \\quad \\forall i \\in \\{1, \\dots, K\\}\n",
    "\\end{align*}}\n",
    "$$\n",
    "where the objective function is a Cobb-Douglas utility function representing the effectiveness of the drug cocktail, and the constraints ensure that the cocktail stays within budget and safety limits. The design variables are the concentrations of each drug in the cocktail, denoted by $n_i$ for drug $i$ (units: mg/kg), $K$ is the total number of available drugs, and $W$ is the weight of the patient (units: kg). The $\\kappa(\\gamma)$ term is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\kappa(\\gamma) & = \\begin{cases}\n",
    "-1 & \\text{if any } \\gamma_i < 0 \\\\\n",
    "1 & \\text{if all } \\gamma_i \\geq 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where coefficients $\\gamma_i$ represent the efficacy of each drug type, which must be learned from historical data or set based on expert knowledge.\n",
    "\n",
    "The first set of constraints ensures that the total cost of the drug cocktail does not exceed a specified budget $B$, where $c_i$ is the cost per unit concentration (e.g., USD/mg) of drug $i$. The second set of constraints ensures that the total concentration of all drugs in the cocktail does not exceed a safety limit $S$. Finally, each drug concentration is bounded by its minimum and maximum allowable levels, denoted by $n_i^{\\text{min}}$ and $n_i^{\\text{max}}$, respectively.\n",
    "\n",
    "In this lab, we'll implement the budget constraint only.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407178e",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "Q-learning iteratively estimates the state action-value function $Q(s, a)$ by conducting repeated experiments $t=1,2,\\ldots$ in the world $\\mathcal{W}$. \n",
    "In each experiment, an agent in state $s\\in\\mathcal{S}$ takes action $a\\in\\mathcal{A}$, receives a reward $r$, and (potentially) transitions to a new state $s^{\\prime}$. After each experiment $t$, the agent updates its estimate of $Q(s, a)$ using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q_{t+1}(s,a)\\leftarrow{\\underbrace{Q_{t}(s,a)}_{\\text{old value}}}+\\alpha_{t}\\cdot\\underbrace{\\left(r+\\gamma\\cdot\\max_{a^{\\prime}\\in\\mathcal{A}}Q_{t}(s^{\\prime},a^{\\prime}) - Q_{t}(s,a)\\right)}_{\\text{new value}}\\quad{t = 1,2,3,\\ldots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $0<\\alpha_{t} <{1}$ is the learning rate parameter at time $t$, and $0<\\gamma<{1}$ is the discount factor. \n",
    "We estimate the policy function $\\pi:\\mathcal{S}\\rightarrow\\mathcal{A}$ by selecting the action $a$ that maximizes $Q(s,a)$ at each state $s$:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi(s) = \\arg\\max_{a\\in\\mathcal{A}}Q(s,a)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Algorithm\n",
    "Initialize $Q(s,a)$ arbitrarily for all $s\\in\\mathcal{S}$, and $a\\in\\mathcal{A}$.\n",
    "Set the hyperparameters: learning rate $\\alpha_{t}$, the discount factor $\\gamma$, the exploration rate $\\epsilon_{t}$, the maximum number of iterations $\\texttt{maxiter}$, and the convergence tolerance $\\delta$. Set $\\texttt{converged}\\gets\\texttt{false}$. \n",
    "\n",
    "For $s\\in\\mathcal{S}$\n",
    "1. Initialize the trial counter $t\\gets{1}$\n",
    "2. While $\\texttt{converged} $ is $\\texttt{false}$ __do__:\n",
    "    1. Roll a random number $p\\in[0,1]$. Compute $\\epsilon_{t}={t^{-1/3}}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ where $K=|\\mathcal{A}|$ is the number of actions.\n",
    "    2. If $p\\leq\\epsilon_{t}$, choose a random (uniform) action $a_{t}\\in\\mathcal{A}$. Otherwise, choose a greedy action $a_{t} = \\text{arg}\\max_{a\\in\\mathcal{A}}{Q_{t}(s,a)}$.\n",
    "    3. Take action $a_{t}$, observe the reward $r$ from the __world__ and transition to the next state $s^{\\prime}$.\n",
    "    4. Update the state-action-value function: $Q_{t+1}(s,a)\\leftarrow{Q_{t}(s,a)}+\\alpha_{t}\\cdot\\underbrace{\\left(r+\\gamma\\cdot\\overbrace{\\max_{a^{\\prime}\\in\\mathcal{A}}Q_{t}(s^{\\prime},a^{\\prime})}^{\\text{one-step lookahead}} - Q_{t}(s,a)\\right)}_{\\text{new information}}$.\n",
    "    5. Update the state $s\\leftarrow{s^{\\prime}}$, the learning rate $\\alpha_{t+1}\\leftarrow\\alpha_{t}$, and the counter $t\\leftarrow{t+1}$\n",
    "    6. Convergence check: If $Q(s,a)$ has bounded change $\\lVert{Q_{t+1}(s,a) - Q_{t}(s,a)}\\rVert\\leq\\delta$, then the algorithm has converged. Set $\\texttt{converged}\\gets\\texttt{true}$.\n",
    "    7. Otherwise: if $t\\geq\\texttt{maxiter}$, then set $\\texttt{converged}\\gets\\texttt{true}$ and notify the caller that the maximum iteration limit was reached without convergence. Proceed to next state.\n",
    "    8. Otherwise: continue to the next iteration.\n",
    "3. End While\n",
    "4. End For\n",
    "\n",
    "### Convergence\n",
    "Q-learning converges to the optimal policy under two key theoretical conditions (assuming the Markov property holds for the environment):\n",
    "* __Learning rate decay__: The learning rate $\\alpha_{t}$ must satisfy $\\sum_{t=0}^\\infty \\alpha_t(s, a) = \\infty$ and $\\sum_{t=0}^\\infty \\alpha_t^2(s, a) < \\infty$ for all state-action pairs, ensuring sufficient initial updates while stabilizing over time. Setting $\\alpha_{t+1} \\gets \\beta\\alpha_{t}$ where $\\beta<1$ is a common choice.\n",
    "* __Infinite exploration__: All state-action pairs must be visited infinitely often. This condition holds for $\\epsilon$-greedy policies with persistent exploration, i.e., $\\epsilon_{t} > 0\\,\\,\\forall{t}$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172ddc3",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdd3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-student.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf7cfd",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e8ea5",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Let's implement the `experiment(...)` function that is used by our reinforcement learning agent to evaluate drug cocktail designs. This function will compute the utility of a given drug cocktail based on the Cobb-Douglas utility function and apply the necessary constraints.\n",
    "\n",
    "> __What is going on in the `experiment(...)` function?__\n",
    "> \n",
    "> This function takes in a context object, which contains information about the drug types, their costs, and levels. It also takes in two integers, `s` and `a`, which represent the state and action, respectively. The function computes the utility of the drug cocktail based on the Cobb-Douglas utility function and checks if the cocktail meets the budget and safety constraints. If the constraints are met, it returns the computed utility; otherwise, it returns a penalty value.\n",
    "\n",
    "Implement the `experiment(...)` function in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0310757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiment (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function experiment(context::MyExperimentalDrugCocktailContext, s::Int64, a::Int64)\n",
    "    \n",
    "    # TODO: Implement the experiment function to compute the utility of giving drug cocktail `a` with dose `s`\n",
    "    throw(ErrorException(\"Oooops! The experiment function has not been implemented\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d58131",
   "metadata": {},
   "source": [
    "### Constants\n",
    "In this section, let's define some constants that will be used in our drug cocktail design problem. These constants include the number of drug types, their costs, and the levels of drug concentrations. See the comment next to each constant for its units, permitted values, and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3; # number of drug types\n",
    "m = 3;  # number of drug levels per type {high, nominal, low}\n",
    "ð’œ = range(1, stop=2^K, step=1) |> collect; # action space: 2^K possible drug combinations (binary selection vector)\n",
    "ð’® = range(1, stop=m^K, step=1) |> collect; # state space: m^K possible dosage level configurations\n",
    "W = 80.0; # weight of the patient in kg\n",
    "B = 1000.0; # budget in USD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688250f6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1b724",
   "metadata": {},
   "source": [
    "## Task 1: Setup the context model\n",
    "In this task, we will set up the context model for our drug cocktail design problem. This involves defining a mutable struct that holds the necessary information about the drugs, their costs, and levels. \n",
    "\n",
    "> __Context model explanation:__ The context model stores the problem structure including the number of drug types `K`, the number of dosage levels `m` per drug, the drug effectiveness coefficients `Î³` (which the agent does not know a priori), the unit costs per drug, the three dosage concentration levels (high, nominal, low) for each drug in mg/kg, the patient weight `W` in kg, and the total budget constraint `B` in USD. This encapsulates all the problem parameters that define the drug cocktail design environment.\n",
    "\n",
    "We save the context information in the `contextmodel::MyExperimentalDrugCocktailContext` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ffe604",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextmodel = let \n",
    "\n",
    "    # initialize - \n",
    "    costs = Dict{Int64, Float64}(); # cost per unit concentration (e.g., USD/mg) of drug i\n",
    "    levels = Dict{Int64, NamedTuple}(); # levels of drug concentrations for drug i\n",
    "    Î³ = Array{Float64}(undef, K); # effectiveness coefficients for each drug type\n",
    "\n",
    "    # generate random cost data -\n",
    "    for i âˆˆ 1:K\n",
    "        costs[i] = rand(0.1:0.1:1.0); # random cost between 0.1 and 1.0 USD/mg\n",
    "    end\n",
    "\n",
    "    # generate random level data -\n",
    "    for i âˆˆ 1:K\n",
    "        high = rand(5.0:1.0:10.0);    # high concentration level in mg/kg\n",
    "        nominal = rand(2.0:0.1:4.9);  # nominal concentration level in mg/kg\n",
    "        low = rand(1.0:0.1:1.9);      # low concentration level in mg/kg\n",
    "        levels[i] = (high=high, nominal=nominal, low=low);\n",
    "    end\n",
    "\n",
    "    # generate effectiveness coefficients -\n",
    "    for i âˆˆ 1:K\n",
    "        Î³[i] = randn(); # random normal efficacy value (negative: inhibitory, positive: excitatory)\n",
    "    end\n",
    "\n",
    "    # build the model -\n",
    "    model = build(MyExperimentalDrugCocktailContext, (\n",
    "\n",
    "        K = K, # number of drug types\n",
    "        m = m, # number of drug levels per type\n",
    "        Î³ = Î³, # effectiveness coefficients for each drug type (we don't know these a priori)\n",
    "        B = B, # total budget in USD\n",
    "        cost = costs, # cost per unit concentration (e.g., USD/mg) of drug i\n",
    "        levels = levels, # levels of drug concentrations for drug i\n",
    "        W = W # weight of the patient in kg\n",
    "    ));\n",
    "    \n",
    "    model; # return the model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb9536",
   "metadata": {},
   "source": [
    "Let's examine the randomly generated problem parameters for our drug cocktail design instance. This table displays the unit costs, dosage concentration levels, and effectiveness coefficients for each drug type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c97063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- ----------- ---------- -------------- ----------- ----------\n",
      " \u001b[1m  drug \u001b[0m \u001b[1m unit_cost \u001b[0m \u001b[1m low_conc \u001b[0m \u001b[1m nominal_conc \u001b[0m \u001b[1m high_conc \u001b[0m \u001b[1m        Î³ \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m      Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m\n",
      " ------- ----------- ---------- -------------- ----------- ----------\n",
      "      1         0.8        1.2            2.4        10.0   0.151504\n",
      "      2         0.1        1.0            2.4         9.0   0.158682\n",
      "      3         0.5        1.2            4.0         9.0   -1.53923\n",
      " ------- ----------- ---------- -------------- ----------- ----------\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    \n",
    "    # extract data from context model -\n",
    "    for i âˆˆ 1:K\n",
    "        row_df = (\n",
    "            drug = i,\n",
    "            unit_cost = contextmodel.cost[i],\n",
    "            low_conc = contextmodel.levels[i].low,\n",
    "            nominal_conc = contextmodel.levels[i].nominal,\n",
    "            high_conc = contextmodel.levels[i].high,\n",
    "            Î³ = contextmodel.Î³[i]\n",
    "        );\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "    \n",
    "    # display the table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4198fbc",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d632eb0",
   "metadata": {},
   "source": [
    "## Task 2: Construct the Learning Agent Model\n",
    "In this task, we will construct the learning agent model that evaluates different drug cocktail designs. This agent will run experiments using the `experiment(...)` function we implemented earlier and will learn to optimize the drug cocktail design over time to maximize utility while adhering to the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383fe3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyQLearningAgentModel([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  â€¦  18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [1, 2, 3, 4, 5, 6, 7, 8], 0.95, 0.1, [0.0 0.0 â€¦ 0.0 0.0; 0.0 0.0 â€¦ 0.0 0.0; â€¦ ; 0.0 0.0 â€¦ 0.0 0.0; 0.0 0.0 â€¦ 0.0 0.0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylearningagent = let\n",
    "\n",
    "    # initialize -\n",
    "    Î³ = 0.95; # discount factor (for future rewards)\n",
    "    Î± = 0.1;  # learning rate\n",
    "    Q = Array{Float64}(undef, length(ð’®), length(ð’œ)); # Q-value table\n",
    "\n",
    "    # fill the Q-table with zeros -\n",
    "    fill!(Q, 0.0); # fast way to fill an array with a value\n",
    "\n",
    "    # build the learning agent model -\n",
    "    model = build(MyQLearningAgentModel, (\n",
    "        Î³ = Î³, # discount factor\n",
    "        Î± = Î±, # learning rate\n",
    "        Q = Q, # Q-value table\n",
    "        states = ð’®, # state space\n",
    "        actions = ð’œ # action space\n",
    "    ));\n",
    "\n",
    "    model; # return the model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c4f81",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740671",
   "metadata": {},
   "source": [
    "## Task 3: Let's let the agent learn!\n",
    "In this task, we will allow the learning agent to interact with the environment and learn from its experiences. The agent will use the `experiment(...)` function to evaluate different drug cocktail designs and update its policy based on the observed rewards. \n",
    "\n",
    "> __What is going on in this code cell?__ We call the `solve(...)` method with our Q-learning agent, the context model, and the `experiment(...)` function as the world model. The agent will run up to 10,000 iterations, exploring different state-action pairs using epsilon-greedy selection. For each iteration, it takes an action (selecting drugs and dosages), observes the reward from the `experiment(...)` function (utility minus budget violation penalties), and updates the Q-value table using the temporal difference learning rule. The algorithm terminates when the maximum change in Q-values falls below the convergence tolerance Î´ = 0.0001 or when the maximum number of iterations is reached.\n",
    "\n",
    "The `result` variable stores the trained Q-learning model containing the converged Q-value table `result.Q`, which encodes the expected cumulative reward for each state-action pair after learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f29c5ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Oooops! The experiment function has not been implemented",
     "output_type": "error",
     "traceback": [
      "Oooops! The experiment function has not been implemented",
      "",
      "Stacktrace:",
      " [1] \u001b[0m\u001b[1mexperiment\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mcontext\u001b[39m::\u001b[0mMyExperimentalDrugCocktailContext, \u001b[90ms\u001b[39m::\u001b[0mInt64, \u001b[90ma\u001b[39m::\u001b[0mInt64\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[8]:4\u001b[24m\u001b[39m",
      " [2] \u001b[0m\u001b[1msolve\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90magent\u001b[39m::\u001b[0mMyQLearningAgentModel, \u001b[90menvironment\u001b[39m::\u001b[0mMyExperimentalDrugCocktailContext; \u001b[90mmaxsteps\u001b[39m::\u001b[0mInt64, \u001b[90mÎ´\u001b[39m::\u001b[0mFloat64, \u001b[90mworldmodel\u001b[39m::\u001b[0mtypeof(experiment)\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m~/Desktop/julia_work/CHEME-5800-Fall-2025/CHEME-5800-Labs-Fall-2025/labs/week-13/L13b/solution/\u001b[39m\u001b[90m\u001b[4mCompute.jl:48\u001b[24m\u001b[39m",
      " [3] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[21]:1\u001b[24m\u001b[39m",
      " [4] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mm\u001b[39m::\u001b[0mModule, \u001b[90me\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:489\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "result = solve(mylearningagent, contextmodel; \n",
    "    maxsteps = 10000, Î´ = 0.0001, worldmodel = experiment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13dc1ef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[22]:1\u001b[24m\u001b[39m",
      " [2] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mm\u001b[39m::\u001b[0mModule, \u001b[90me\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:489\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "result.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6d35c",
   "metadata": {},
   "source": [
    "Let's extract the policy from our trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa135fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ï€(s) = mypolicy(result.Q)[s]; # define a policy function that returns the best action for state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f30bcf34",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
      "",
      "Stacktrace:",
      " [1] \u001b[0m\u001b[1mÏ€\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mInt64\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[24]:1\u001b[24m\u001b[39m",
      " [2] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[25]:1\u001b[24m\u001b[39m",
      " [3] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mm\u001b[39m::\u001b[0mModule, \u001b[90me\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:489\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "Ï€(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ae003",
   "metadata": {},
   "source": [
    "What's in the policy? Let's pick the best action for each state according to the learned Q-values. We'll display these in a table.\n",
    "\n",
    "> __What's going on in this table?__ For a given state `s` (representing a specific dosage level configuration), we extract the optimal action `a` from the learned policy. We then decode the action into a binary vector to identify which drugs are selected, and decode the state to determine the dosage level (high, nominal, or low) for each selected drug. \n",
    "> \n",
    "> The table displays the drug index, dosage level, unit cost, concentration levels available, total spending for that drug (concentration Ã— patient weight Ã— unit cost), the effectiveness coefficient Î³, and the Q-value for this state-action pair. This shows the recommended drug cocktail composition that maximizes expected utility while respecting budget constraints for each state (dose combinations).\n",
    "\n",
    "The table reveals which drugs the agent recommends including in the cocktail, at which dosage levels, and the expected value of following this recommendation from the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "253768a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `result` not defined in `Main`\nSuggestion: add an appropriate import or assignment. This global was declared but not assigned.",
      "",
      "Stacktrace:",
      " [1] \u001b[0m\u001b[1mÏ€\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mInt64\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[24]:1\u001b[24m\u001b[39m",
      " [2] top-level scope",
      "\u001b[90m   @\u001b[39m \u001b[90m\u001b[4mIn[27]:6\u001b[24m\u001b[39m",
      " [3] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mm\u001b[39m::\u001b[0mModule, \u001b[90me\u001b[39m::\u001b[0mAny\u001b[0m\u001b[1m)\u001b[22m",
      "\u001b[90m   @\u001b[39m \u001b[90mCore\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mboot.jl:489\u001b[24m\u001b[39m"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    s = 10; # what state do we want to look at?\n",
    "    a = Ï€(s); # this gives me the best action *index* for state s\n",
    "    N = 2^K; # number of actions\n",
    "    M = m^K; # number of states\n",
    "    Î³ = contextmodel.Î³; # effectiveness coefficients for each drug type\n",
    "\n",
    "    aáµ¢ = digits(a, base=2, pad=K); # action vector representation\n",
    "    if (a == N)\n",
    "        aáµ¢ = digits(a-1, base=2, pad=K); # adjust for all-drug case\n",
    "    end\n",
    "    Sâ‚Š = findall(x -> x == 1, aáµ¢); # indices of drugs being administered\n",
    "\n",
    "    for i âˆˆ eachindex(Sâ‚Š)\n",
    "        drug_index = Sâ‚Š[i];\n",
    "        level_index = digits(s, base=m, pad=K)[drug_index];\n",
    "\n",
    "        level_str = \"\";\n",
    "        if (level_index == 0)\n",
    "            level_str = \"high\";\n",
    "        elseif (level_index == 1)\n",
    "            level_str = \"nominal\";\n",
    "        elseif (level_index == 2)\n",
    "            level_str = \"low\";\n",
    "        end\n",
    "        n = contextmodel.levels[drug_index];\n",
    "        spend = 0.0;\n",
    "        if level_index == 0\n",
    "            spend = (W*n.high)*contextmodel.cost[drug_index];\n",
    "        elseif level_index == 1\n",
    "            spend = (W*n.nominal)*contextmodel.cost[drug_index];\n",
    "        elseif level_index == 2\n",
    "            spend = (W*n.low)*contextmodel.cost[drug_index];\n",
    "        end\n",
    "\n",
    "        push!(df, (\n",
    "            state = s,\n",
    "            action = a,\n",
    "            drug = drug_index,\n",
    "            level = level_str,\n",
    "            unitcost = contextmodel.cost[drug_index],\n",
    "            concentration = n,\n",
    "            spend = spend,\n",
    "            Î³ = Î³[drug_index],\n",
    "            q_value = result.Q[s, a]\n",
    "        ));\n",
    "    end\n",
    "\n",
    "    # make a pretty table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        fit_table_in_display_horizontally = false,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407dbc4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e12ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we applied Q-learning to solve the drug cocktail design problem, learning optimal policies for selecting drug combinations and dosage levels under budget constraints.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **Q-learning for combinatorial optimization**: We implemented a Q-learning agent that iteratively estimates state-action value functions through temporal difference updates, converging to an optimal policy without requiring explicit knowledge of transition probabilities or reward functions. The algorithm balanced exploration (trying new drug combinations) with exploitation (selecting known effective combinations) using an epsilon-greedy strategy with decaying exploration rate.\n",
    "> * **MDP formulation for drug design**: We encoded the drug cocktail problem as a Markov decision process with states representing dosage level configurations (high, nominal, low for each drug type), actions representing binary drug selection vectors, and rewards computed from Cobb-Douglas utility functions penalized for budget violations. This formulation captured both therapeutic effectiveness (through preference parameters Î³áµ¢) and resource constraints (through budget limits).\n",
    "> * **Policy extraction and interpretation**: We derived the optimal policy by selecting argmax actions from the converged Q-value table, mapping each state to the drug combination that maximizes expected cumulative reward. The learned policy encoded both drug selection decisions (which drugs to include) and dosage optimization (which concentration levels to use), demonstrating how reinforcement learning discovers structured solutions to constrained combinatorial problems.\n",
    "\n",
    "Q-learning provides a framework for sequential decision-making in optimization problems where the relationship between actions and outcomes must be learned through experience rather than analytical derivation.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
